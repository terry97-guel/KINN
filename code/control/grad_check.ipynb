{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Jupyter...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/taerim/PRIMNET-V2/code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"PRIMNET/FINGER\"\n",
    "\n",
    "from path_handler import get_BASERDIR\n",
    "import os\n",
    "BASEDIR, RUNMODE = get_BASERDIR(\".\")\n",
    "\n",
    "if BASEDIR.absolute().name == \"control\":\n",
    "    os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "try: \n",
    "    get_ipython().__class__.__name__\n",
    "    BASEDIR = Path().absolute()\n",
    "except: BASEDIR = Path(__file__).parent\n",
    "\n",
    "sys.path.append(str(BASEDIR))\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from utils import dataloader\n",
    "from utils.initalize import INITALZE_EVEN_JOINTS\n",
    "from utils.update import  update_primnet, update_fc_primnet, update_pcc_primnet\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import time\n",
    "import json\n",
    "\n",
    "from utils.tools import set_seed, set_wandb, print_log_dict, prefix_dict, average_dict\n",
    "from utils.path_handler import JUPYTER, RUN, DEBUG, get_BASERDIR\n",
    "from utils.args import read_ARGS\n",
    "from utils.logger import CSVLogger,ask_and_make_folder\n",
    "from utils.tools import cast_numpy\n",
    "\n",
    "from configs.template import PRIMNET_ARGS_TEMPLATE, FC_PRIMNET_ARGS_TEMPLATE, PCC_PRIMNET_ARGS_TEMPLATE\n",
    "from model.PRIMNET import PRIMNET\n",
    "from model.FC_PRIMNET import FC_PRIMNET\n",
    "from typing import Union\n",
    "\n",
    "from utils.dataloader import get_dataset, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Jupyter...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ARGS(MODEL='PRIMNET', EVEN_JOINTS=True, WANDB=True, pname='PRIMNET_v2.1', runname='FINGER', DATASET='FINGER', TPOSE=((0, 0, 0.12),), LOAD_WEIGHTPATH=None, SAVE_PERIOD=1, TEST_PERIOD=1, EVEN_JOINT=True, p_offset_std=0.1, rpy_offset_std=0.01, axis_std=0.1, OUTPUT_NORMALIZE=False, seed=0, hdim=(16, 16), motor_embed_dim=4, lr=0.0015, lrd=0.95, wd=0.0, w_vec=0.001, epochs=500, focus_ratio=0.0, data_ratio=1.0, n_workers=2, batch_size=64, joint_seqs=('F', 'R', 'P', 'R', 'R', 'P', 'R'), marker_num=1, motor_dim=4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASEDIR, RUNMODE = get_BASERDIR(os.getcwd())\n",
    "args = read_ARGS((BASEDIR/'results'/path/\"args.py\").absolute())\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PRIMNET(args=args).to(args.device)\n",
    "\n",
    "model.load_state_dict(torch.load(BASEDIR/'results'/path/\"weights/epoch_500.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_motor(model:PRIMNET, motor_control):     \n",
    "    motor_control = model.normalize(motor_control)\n",
    "    \n",
    "    # Forward\n",
    "    act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "    q_values = model.FK_LAYER.forward_q(act_embeds)\n",
    "    joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "    \n",
    "    return model.t2p(joint_se3, OUTPUT_NORMALIZE=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinematic gradient - Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import _vmap,fwAD, _as_tuple, _grad_preprocess, _check_requires_grad, _construct_standard_basis_for, _autograd_grad, _grad_postprocess, _tuple_postprocess\n",
    "\n",
    "def _jacfwd(func, inputs, strict=False, vectorize=False):\n",
    "    if strict:\n",
    "        raise RuntimeError('torch.autograd.functional.jacobian: `strict=True` '\n",
    "                           'and `strategy=\"forward-mode\"` are not supported together (yet). '\n",
    "                           'Please either set `strict=False` or '\n",
    "                           '`strategy=\"reverse-mode\"`.')\n",
    "    is_inputs_tuple, inputs = _as_tuple(inputs, \"inputs\", \"jacobian\")\n",
    "    output_info = []\n",
    "\n",
    "    if vectorize:\n",
    "        # See NOTE: [Computing jacobian with vmap and grad for multiple outputs]\n",
    "        input_numels = tuple(input.numel() for input in inputs)\n",
    "\n",
    "        # Step 1: Prepare tangents\n",
    "        tangents = _construct_standard_basis_for(inputs, input_numels)\n",
    "\n",
    "        # Step 2: Compute vmap over computation with dual tensors\n",
    "        def jvp(tangents):\n",
    "            with fwAD.dual_level():\n",
    "                dual_inputs = tuple(\n",
    "                    fwAD.make_dual(input, tangent.view_as(input)) for input, tangent in zip(inputs, tangents))\n",
    "                _is_outputs_tuple, dual_outputs = _as_tuple(func(*dual_inputs), \"outputs\")\n",
    "                output_info.append(_is_outputs_tuple)\n",
    "                jv = []\n",
    "                primal_outs = []\n",
    "                for dual_out in dual_outputs:\n",
    "                    primal, tangent = fwAD.unpack_dual(dual_out)\n",
    "                    primal_outs.append(primal)\n",
    "                    if tangent is not None:\n",
    "                        jv.append(tangent)\n",
    "                    else:\n",
    "                        jv.append(torch.zeros_like(primal))\n",
    "                output_info.append(primal_outs)\n",
    "                return tuple(jv)\n",
    "\n",
    "        outputs_before_split = _vmap(jvp)(tangents)\n",
    "        is_outputs_tuple, outputs = output_info\n",
    "        # Step 3: for each of the output tangents, split along dim 0\n",
    "        jacobian_input_output = []\n",
    "        for jac, output_i in zip(outputs_before_split, outputs):\n",
    "            jacobian_output_i_output = []\n",
    "            for jac, input_j in zip(jac.split(input_numels, dim=0), inputs):\n",
    "                # We need to transpose the Jacobian because in forward AD, the\n",
    "                # batch dimension represents that of the inputs\n",
    "                jacobian_input_i_output_j = jac.permute(*range(1, jac.ndim), 0) \\\n",
    "                    .reshape(tuple([*output_i.shape, *input_j.shape]))  # noqa: C409\n",
    "\n",
    "                jacobian_output_i_output.append(jacobian_input_i_output_j)\n",
    "            jacobian_input_output.append(jacobian_output_i_output)\n",
    "\n",
    "        # Omit [Step 4] because everything is already transposed w/ forward AD\n",
    "        return _tuple_postprocess(jacobian_input_output, (is_outputs_tuple, is_inputs_tuple))\n",
    "    else:\n",
    "        raise NotImplementedError(\"Computing Jacobian using forward-AD or forward-over-reverse Hessian is\"\n",
    "                                  \"only implemented for `vectorize=True`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import _jacfwd, _as_tuple, _grad_preprocess, _check_requires_grad, _construct_standard_basis_for, _autograd_grad, _grad_postprocess, _tuple_postprocess\n",
    "\n",
    "from typing import List, Tuple\n",
    "def jacobian(func, inputs, create_graph=False, strict=False, vectorize=False, strategy=\"reverse-mode\"):\n",
    "    r\"\"\"Function that computes the Jacobian of a given function.\n",
    "\n",
    "    Args:\n",
    "        func (function): a Python function that takes Tensor inputs and returns\n",
    "            a tuple of Tensors or a Tensor.\n",
    "        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.\n",
    "        create_graph (bool, optional): If ``True``, the Jacobian will be\n",
    "            computed in a differentiable manner. Note that when ``strict`` is\n",
    "            ``False``, the result can not require gradients or be disconnected\n",
    "            from the inputs.  Defaults to ``False``.\n",
    "        strict (bool, optional): If ``True``, an error will be raised when we\n",
    "            detect that there exists an input such that all the outputs are\n",
    "            independent of it. If ``False``, we return a Tensor of zeros as the\n",
    "            jacobian for said inputs, which is the expected mathematical value.\n",
    "            Defaults to ``False``.\n",
    "        vectorize (bool, optional): This feature is experimental.\n",
    "            Please consider using\n",
    "            `functorch's jacrev or jacfwd <https://github.com/pytorch/functorch#what-are-the-transforms>`_\n",
    "            instead if you are looking for something less experimental and more performant.\n",
    "            When computing the jacobian, usually we invoke\n",
    "            ``autograd.grad`` once per row of the jacobian. If this flag is\n",
    "            ``True``, we perform only a single ``autograd.grad`` call with\n",
    "            ``batched_grad=True`` which uses the vmap prototype feature.\n",
    "            Though this should lead to performance improvements in many cases,\n",
    "            because this feature is still experimental, there may be performance\n",
    "            cliffs. See :func:`torch.autograd.grad`'s ``batched_grad`` parameter for\n",
    "            more information.\n",
    "        strategy (str, optional): Set to ``\"forward-mode\"`` or ``\"reverse-mode\"`` to\n",
    "            determine whether the Jacobian will be computed with forward or reverse\n",
    "            mode AD. Currently, ``\"forward-mode\"`` requires ``vectorized=True``.\n",
    "            Defaults to ``\"reverse-mode\"``. If ``func`` has more outputs than\n",
    "            inputs, ``\"forward-mode\"`` tends to be more performant. Otherwise,\n",
    "            prefer to use ``\"reverse-mode\"``.\n",
    "\n",
    "    Returns:\n",
    "        Jacobian (Tensor or nested tuple of Tensors): if there is a single\n",
    "        input and output, this will be a single Tensor containing the\n",
    "        Jacobian for the linearized inputs and output. If one of the two is\n",
    "        a tuple, then the Jacobian will be a tuple of Tensors. If both of\n",
    "        them are tuples, then the Jacobian will be a tuple of tuple of\n",
    "        Tensors where ``Jacobian[i][j]`` will contain the Jacobian of the\n",
    "        ``i``\\th output and ``j``\\th input and will have as size the\n",
    "        concatenation of the sizes of the corresponding output and the\n",
    "        corresponding input and will have same dtype and device as the\n",
    "        corresponding input. If strategy is ``forward-mode``, the dtype will be\n",
    "        that of the output; otherwise, the input.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> def exp_reducer(x):\n",
    "        ...   return x.exp().sum(dim=1)\n",
    "        >>> inputs = torch.rand(2, 2)\n",
    "        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
    "        >>> jacobian(exp_reducer, inputs)\n",
    "        tensor([[[1.4917, 2.4352],\n",
    "                 [0.0000, 0.0000]],\n",
    "                [[0.0000, 0.0000],\n",
    "                 [2.4369, 2.3799]]])\n",
    "\n",
    "        >>> jacobian(exp_reducer, inputs, create_graph=True)\n",
    "        tensor([[[1.4917, 2.4352],\n",
    "                 [0.0000, 0.0000]],\n",
    "                [[0.0000, 0.0000],\n",
    "                 [2.4369, 2.3799]]], grad_fn=<ViewBackward>)\n",
    "\n",
    "        >>> def exp_adder(x, y):\n",
    "        ...   return 2 * x.exp() + 3 * y\n",
    "        >>> inputs = (torch.rand(2), torch.rand(2))\n",
    "        >>> jacobian(exp_adder, inputs)\n",
    "        (tensor([[2.8052, 0.0000],\n",
    "                [0.0000, 3.3963]]),\n",
    "         tensor([[3., 0.],\n",
    "                 [0., 3.]]))\n",
    "    \"\"\"\n",
    "    assert strategy in (\"forward-mode\", \"reverse-mode\"), (\n",
    "        'Expected strategy to be either \"forward-mode\" or \"reverse-mode\". Hint: If your '\n",
    "        'function has more outputs than inputs, \"forward-mode\" tends to be more performant. '\n",
    "        'Otherwise, prefer to use \"reverse-mode\".')\n",
    "    if strategy == \"forward-mode\":\n",
    "        if create_graph:\n",
    "            raise NotImplementedError('torch.autograd.functional.jacobian: `create_graph=True` '\n",
    "                                      'and `strategy=\"forward-mode\"` are not supported together (yet). '\n",
    "                                      'Please either set `create_graph=False` or '\n",
    "                                      '`strategy=\"reverse-mode\"`.')\n",
    "        return _jacfwd(func, inputs, strict, vectorize)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        is_inputs_tuple, inputs = _as_tuple(inputs, \"inputs\", \"jacobian\")\n",
    "        inputs = _grad_preprocess(inputs, create_graph=create_graph, need_graph=True)\n",
    "\n",
    "        outputs = func(*inputs)\n",
    "        is_outputs_tuple, outputs = _as_tuple(outputs,\n",
    "                                              \"outputs of the user-provided function\",\n",
    "                                              \"jacobian\")\n",
    "        _check_requires_grad(outputs, \"outputs\", strict=strict)\n",
    "\n",
    "        if vectorize:\n",
    "            if strict:\n",
    "                raise RuntimeError('torch.autograd.functional.jacobian: `strict=True` '\n",
    "                                   'and `vectorized=True` are not supported together. '\n",
    "                                   'Please either set `strict=False` or '\n",
    "                                   '`vectorize=False`.')\n",
    "            # NOTE: [Computing jacobian with vmap and grad for multiple outputs]\n",
    "            #\n",
    "            # Let's consider f(x) = (x**2, x.sum()) and let x = torch.randn(3).\n",
    "            # It turns out we can compute the jacobian of this function with a single\n",
    "            # call to autograd.grad by using vmap over the correct grad_outputs.\n",
    "            #\n",
    "            # Firstly, one way to compute the jacobian is to stack x**2 and x.sum()\n",
    "            # into a 4D vector. E.g., use g(x) = torch.stack([x**2, x.sum()])\n",
    "            #\n",
    "            # To get the first row of the jacobian, we call\n",
    "            # >>> autograd.grad(g(x), x, grad_outputs=torch.tensor([1, 0, 0, 0]))\n",
    "            # To get the 2nd row of the jacobian, we call\n",
    "            # >>> autograd.grad(g(x), x, grad_outputs=torch.tensor([0, 1, 0, 0]))\n",
    "            # and so on.\n",
    "            #\n",
    "            # Using vmap, we can vectorize all 4 of these computations into one by\n",
    "            # passing the standard basis for R^4 as the grad_output.\n",
    "            # vmap(partial(autograd.grad, g(x), x))(torch.eye(4)).\n",
    "            #\n",
    "            # Now, how do we compute the jacobian *without stacking the output*?\n",
    "            # We can just split the standard basis across the outputs. So to\n",
    "            # compute the jacobian of f(x), we'd use\n",
    "            # >>> autograd.grad(f(x), x, grad_outputs=_construct_standard_basis_for(...))\n",
    "            # The grad_outputs looks like the following:\n",
    "            # ( torch.tensor([[1, 0, 0],\n",
    "            #                 [0, 1, 0],\n",
    "            #                 [0, 0, 1],\n",
    "            #                 [0, 0, 0]]),\n",
    "            #   torch.tensor([[0],\n",
    "            #                 [0],\n",
    "            #                 [0],\n",
    "            #                 [1]]) )\n",
    "            #\n",
    "            # But we're not done yet!\n",
    "            # >>> vmap(partial(autograd.grad(f(x), x, grad_outputs=...)))\n",
    "            # returns a Tensor of shape [4, 3]. We have to remember to split the\n",
    "            # jacobian of shape [4, 3] into two:\n",
    "            # - one of shape [3, 3] for the first output\n",
    "            # - one of shape [   3] for the second output\n",
    "\n",
    "            # Step 1: Construct grad_outputs by splitting the standard basis\n",
    "            output_numels = tuple(output.numel() for output in outputs)\n",
    "            grad_outputs = _construct_standard_basis_for(outputs, output_numels)\n",
    "            flat_outputs = tuple(output.reshape(-1) for output in outputs)\n",
    "\n",
    "            # Step 2: Call vmap + autograd.grad\n",
    "            def vjp(grad_output):\n",
    "                vj = list(_autograd_grad(flat_outputs, inputs, grad_output, create_graph=create_graph, is_grads_batched=True))\n",
    "                for el_idx, vj_el in enumerate(vj):\n",
    "                    if vj_el is not None:\n",
    "                        continue\n",
    "                    vj[el_idx] = torch.zeros_like(inputs[el_idx]).expand((sum(output_numels),) + inputs[el_idx].shape)\n",
    "                return tuple(vj)\n",
    "\n",
    "            jacobians_of_flat_output = vjp(grad_outputs)\n",
    "\n",
    "            # Step 3: The returned jacobian is one big tensor per input. In this step,\n",
    "            # we split each Tensor by output.\n",
    "            jacobian_input_output = []\n",
    "            for jac, input_i in zip(jacobians_of_flat_output, inputs):\n",
    "                jacobian_input_i_output = []\n",
    "                for jac, output_j in zip(jac.split(output_numels, dim=0), outputs):\n",
    "                    jacobian_input_i_output_j = jac.view(output_j.shape + input_i.shape)\n",
    "                    jacobian_input_i_output.append(jacobian_input_i_output_j)\n",
    "                jacobian_input_output.append(jacobian_input_i_output)\n",
    "\n",
    "            # Step 4: Right now, `jacobian` is a List[List[Tensor]].\n",
    "            # The outer List corresponds to the number of inputs,\n",
    "            # the inner List corresponds to the number of outputs.\n",
    "            # We need to exchange the order of these and convert to tuples\n",
    "            # before returning.\n",
    "            jacobian_output_input = tuple(zip(*jacobian_input_output))\n",
    "\n",
    "            jacobian_output_input = _grad_postprocess(jacobian_output_input, create_graph)\n",
    "            return _tuple_postprocess(jacobian_output_input, (is_outputs_tuple, is_inputs_tuple))\n",
    "\n",
    "        jacobian: Tuple[torch.Tensor, ...] = tuple()\n",
    "\n",
    "        for i, out in enumerate(outputs):\n",
    "\n",
    "            # mypy complains that expression and variable have different types due to the empty list\n",
    "            jac_i: Tuple[List[torch.Tensor]] = tuple([] for _ in range(len(inputs)))  # type: ignore[assignment]\n",
    "            for j in range(out.nelement()):\n",
    "                vj = _autograd_grad((out.reshape(-1)[j],), inputs,\n",
    "                                    retain_graph=True, create_graph=create_graph)\n",
    "\n",
    "                for el_idx, (jac_i_el, vj_el, inp_el) in enumerate(zip(jac_i, vj, inputs)):\n",
    "                    if vj_el is not None:\n",
    "                        if strict and create_graph and not vj_el.requires_grad:\n",
    "                            msg = (\"The jacobian of the user-provided function is \"\n",
    "                                   \"independent of input {}. This is not allowed in \"\n",
    "                                   \"strict mode when create_graph=True.\".format(i))\n",
    "                            raise RuntimeError(msg)\n",
    "                        jac_i_el.append(vj_el)\n",
    "                    else:\n",
    "                        if strict:\n",
    "                            msg = (\"Output {} of the user-provided function is \"\n",
    "                                   \"independent of input {}. This is not allowed in \"\n",
    "                                   \"strict mode.\".format(i, el_idx))\n",
    "                            raise RuntimeError(msg)\n",
    "                        jac_i_el.append(torch.zeros_like(inp_el))\n",
    "\n",
    "            jacobian += (tuple(torch.stack(jac_i_el, dim=0).view(out.size()  # type: ignore[operator]\n",
    "                         + inputs[el_idx].size()) for (el_idx, jac_i_el) in enumerate(jac_i)), )\n",
    "\n",
    "        jacobian = _grad_postprocess(jacobian, create_graph)\n",
    "\n",
    "        return _tuple_postprocess(jacobian, (is_outputs_tuple, is_inputs_tuple)), outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0052, -0.0211,  0.0104, -0.0003, -0.0076, -0.0046]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motor_control = torch.ones(1,args.motor_dim).to(args.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "    q_values = model.FK_LAYER.forward_q(act_embeds)\n",
    "\n",
    "q_values = q_values.detach()\n",
    "q_values.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import autograd, Tensor\n",
    "\n",
    "def forward_kinematics(q_values:Tensor, idx=-1):\n",
    "    assert q_values.ndim == 2\n",
    "    assert q_values.shape[0] == 1\n",
    "    \n",
    "    joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "    joint_position = model.t2p(joint_se3, OUTPUT_NORMALIZE=False)\n",
    "\n",
    "    # aux_joints = len(args.joint_seqs)//args.marker_num\n",
    "\n",
    "    # marker_position = []\n",
    "    # for i in range(len(args.joint_seqs)):\n",
    "    #     if (i+1)%aux_joints == 0:\n",
    "    #         joint_position_ = joint_position[:,i]\n",
    "    #         marker_position.append(joint_position_[0])\n",
    "\n",
    "    # return torch.stack(marker_position, dim=0)[prim_idxs]\n",
    "    return joint_position[0,idx,:,0]\n",
    "\n",
    "EE_pos = forward_kinematics(q_values, idx=-1)\n",
    "\n",
    "EE_pos.shape, q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 7]),\n",
       " tensor([[ 0.0000,  1.5545,  0.4118,  2.1619, -0.1977, -1.0993,  0.0000],\n",
       "         [ 0.0000,  1.0748,  1.3707,  1.0633,  1.6178, -2.5206,  0.0000],\n",
       "         [ 0.0000,  0.2390,  0.1161,  0.2985,  0.2746,  0.1201,  0.0000]]),\n",
       " (tensor([ 0.0045, -0.0215,  0.1012], grad_fn=<SelectBackward0>),))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "jac_auto, val = jacobian(partial(forward_kinematics, idx=6), q_values)\n",
    "jac_auto = jac_auto[:,0,:]\n",
    "jac_auto.shape, jac_auto, val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinematics gradient - Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.pyart import t2r\n",
    "\n",
    "motor_control = model.normalize(motor_control)\n",
    "\n",
    "# Forward\n",
    "act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "q_values = model.FK_LAYER.forward_q(act_embeds)\n",
    "joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "\n",
    "joint_position =  model.t2p(joint_se3, OUTPUT_NORMALIZE=False)[0,:,:,0]\n",
    "joint_rotation = t2r(joint_se3[0])\n",
    "\n",
    "EE_pos = joint_position[-1]\n",
    "\n",
    "EE_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.PRIMNET import Fjoint, Tjoint, Rjoint, Pjoint\n",
    "\n",
    "jac_explicit = torch.zeros(3, len(model.FK_LAYER.joints))\n",
    "\n",
    "for idx,joint in enumerate(model.FK_LAYER.joints):\n",
    "    if isinstance(joint, Fjoint):\n",
    "        continue\n",
    "    elif isinstance(joint, Tjoint):\n",
    "        continue\n",
    "    elif isinstance(joint, Rjoint):\n",
    "        pos_diff = EE_pos - joint_position[idx]\n",
    "        jac_explicit[:, idx] = torch.cross(joint_rotation[idx] @ joint.axis.data[:,0], pos_diff)\n",
    "        # print('here')\n",
    "    elif isinstance(joint, Pjoint):\n",
    "        pos_diff = EE_pos - joint_position[idx]\n",
    "        jac_explicit[:,idx] = joint_rotation[idx] @ joint.axis.data[:,0]\n",
    "\n",
    "assert torch.sum(jac_explicit - jac_auto) < 1e-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Check - Kinematic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoGrad]:0.794041\n",
      "[Explicit]:0.345913\n",
      "Result: [Explicit] is faster than [AutoGrad] by 2.295493 times\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n = 100\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(n):\n",
    "    jac_auto = jacobian(partial(forward_kinematics, idx=6), q_values)[0]\n",
    "    jac_auto = jac_auto[:,0,:]\n",
    "end_time = time.time()\n",
    "\n",
    "autograd_time = end_time-start_time\n",
    "print(\"[AutoGrad]:{:2f}\".format(autograd_time))\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(n):\n",
    "    jac_explicit = torch.zeros(3, len(model.FK_LAYER.joints))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "        q_values = model.FK_LAYER.forward_q(act_embeds)\n",
    "        joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "\n",
    "        joint_position =  model.t2p(joint_se3, OUTPUT_NORMALIZE=False)[0,:,:,0]\n",
    "\n",
    "        EE_pos = joint_position[-1]\n",
    "\n",
    "    for idx,joint in enumerate(model.FK_LAYER.joints):\n",
    "        if isinstance(joint, Fjoint):\n",
    "            continue\n",
    "        elif isinstance(joint, Tjoint):\n",
    "            continue\n",
    "        elif isinstance(joint, Rjoint):\n",
    "            pos_diff = EE_pos - joint_position[idx]\n",
    "            jac_explicit[:, idx] = torch.cross(joint_rotation[idx] @ joint.axis.data[:,0], pos_diff)\n",
    "            # print('here')\n",
    "        elif isinstance(joint, Pjoint):\n",
    "            pos_diff = EE_pos - joint_position[idx]\n",
    "            jac_explicit[:,idx] = joint_rotation[idx] @ joint.axis.data[:,0]\n",
    "end_time = time.time()\n",
    "explicit_time = end_time-start_time\n",
    "print(\"[Explicit]:{:2f}\".format(explicit_time))\n",
    "\n",
    "print(\"Result: [Explicit] is faster than [AutoGrad] by {:2f} times\".format(autograd_time/explicit_time))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Check - forward w/wo autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [without_grad] is faster than [with_grad] by 1.368354 times\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(n):\n",
    "    forward_kinematics(q_values, idx=6)\n",
    "end_time = time.time()\n",
    "\n",
    "with_grad_time = end_time-start_time\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(n):\n",
    "    with torch.no_grad():\n",
    "        forward_kinematics(q_values, idx=6)\n",
    "end_time = time.time()\n",
    "without_grad_time = end_time-start_time\n",
    "\n",
    "print(\"Result: [without_grad] is faster than [with_grad] by {:2f} times\".format(with_grad_time/without_grad_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full gradient - Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.3906e-05,  1.5433e-05,  2.1243e-05, -3.0690e-05],\n",
       "         [ 8.0282e-06, -5.1830e-06, -7.1341e-06,  1.0307e-05],\n",
       "         [ 2.8990e-06, -1.8716e-06, -2.5761e-06,  3.7217e-06]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motor_control = torch.ones(1,args.motor_dim).to(args.device)\n",
    "\n",
    "\n",
    "def forward_model(model:PRIMNET, motor_control):\n",
    "    motor_control = model.normalize(motor_control)\n",
    "    \n",
    "    # Forward\n",
    "    act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "    q_values = model.FK_LAYER.forward_q(act_embeds)\n",
    "    joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "    \n",
    "    return model.t2p(joint_se3, OUTPUT_NORMALIZE=False)[0,-1,:,0]\n",
    "\n",
    "\n",
    "dp_dm, p = jacobian(partial(forward_model, model), motor_control)\n",
    "dp_dm = dp_dm[:,0,:]\n",
    "dp_dm, dp_dm.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full gradient - Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_q(model:PRIMNET, motor_control):\n",
    "    motor_control = model.normalize(motor_control)\n",
    "\n",
    "    # Forward\n",
    "    act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "    q_values = model.FK_LAYER.forward_q(act_embeds)[0]\n",
    "    return q_values\n",
    "\n",
    "dq_dm, q_values_tuple = jacobian(partial(forward_q, model), motor_control)\n",
    "dq_dm = dq_dm[:,0,:]; q_values = q_values_tuple[0].unsqueeze(0)\n",
    "dq_dm.shape, q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.pyart import t2r\n",
    "joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "\n",
    "joint_position =  model.t2p(joint_se3, OUTPUT_NORMALIZE=False)[0,:,:,0]\n",
    "joint_rotation = t2r(joint_se3[0])\n",
    "\n",
    "EE_pos = joint_position[-1]\n",
    "\n",
    "EE_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3906e-05,  1.5433e-05,  2.1243e-05, -3.0690e-05],\n",
       "        [ 8.0282e-06, -5.1830e-06, -7.1341e-06,  1.0307e-05],\n",
       "        [ 2.8990e-06, -1.8716e-06, -2.5761e-06,  3.7217e-06]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.PRIMNET import Fjoint, Tjoint, Rjoint, Pjoint\n",
    "\n",
    "jac_explicit = torch.zeros(3, len(model.FK_LAYER.joints))\n",
    "\n",
    "for idx,joint in enumerate(model.FK_LAYER.joints):\n",
    "    if isinstance(joint, Fjoint):\n",
    "        continue\n",
    "    elif isinstance(joint, Tjoint):\n",
    "        continue\n",
    "    elif isinstance(joint, Rjoint):\n",
    "        pos_diff = EE_pos - joint_position[idx]\n",
    "        jac_explicit[:, idx] = torch.cross(joint_rotation[idx] @ joint.axis.data[:,0], pos_diff)\n",
    "        # print('here')\n",
    "    elif isinstance(joint, Pjoint):\n",
    "        pos_diff = EE_pos - joint_position[idx]\n",
    "        jac_explicit[:,idx] = joint_rotation[idx] @ joint.axis.data[:,0]\n",
    "\n",
    "dp_dq = jac_explicit\n",
    "\n",
    "dp_dq @ dq_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoGrad]:0.866574\n",
      "[Explicit]:0.516586\n",
      "Result: [Explicit] is faster than [AutoGrad] by 1.677501 times\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n = 100\n",
    "\n",
    "motor_control = torch.zeros(1,args.motor_dim).to(args.device)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for _ in range(n):\n",
    "    dp_dm, p = jacobian(partial(forward_model, model), motor_control)\n",
    "    dp_dm = dp_dm[:,0,:]\n",
    "    \n",
    "    # print(dp_dm)\n",
    "end_time = time.time()\n",
    "\n",
    "autograd_time = end_time-start_time\n",
    "print(\"[AutoGrad]:{:2f}\".format(autograd_time))\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "def forward_q(model:PRIMNET, motor_control):\n",
    "    motor_control = model.normalize(motor_control)\n",
    "\n",
    "    # Forward\n",
    "    act_embeds = model.ACT_EMBED.layers(motor_control)\n",
    "    q_values = model.FK_LAYER.forward_q(act_embeds)[0]\n",
    "    return q_values\n",
    "\n",
    "for _ in range(n):\n",
    "    dq_dm,q_values_tuple = jacobian(partial(forward_q, model), motor_control)\n",
    "    dq_dm = dq_dm[:,0,:].detach(); q_values = q_values_tuple[0].detach().unsqueeze(0)\n",
    "\n",
    "    \n",
    "    @ torch.jit.script_if_tracing\n",
    "    def kinematic_grad(model:PRIMNET, q_values):\n",
    "        with torch.no_grad():\n",
    "            joint_se3 = model.FK_LAYER.forward_kinematics(q_values)\n",
    "\n",
    "            joint_position =  model.t2p(joint_se3, OUTPUT_NORMALIZE=False)[0,:,:,0]\n",
    "            joint_rotation = t2r(joint_se3[0])\n",
    "\n",
    "            EE_pos = joint_position[-1]\n",
    "\n",
    "            jac_explicit = torch.zeros(3, len(model.FK_LAYER.joints))\n",
    "\n",
    "            for idx,joint in enumerate(model.FK_LAYER.joints):\n",
    "                if isinstance(joint, Fjoint):\n",
    "                    continue\n",
    "                elif isinstance(joint, Tjoint):\n",
    "                    continue\n",
    "                elif isinstance(joint, Rjoint):\n",
    "                    pos_diff = EE_pos - joint_position[idx]\n",
    "                    jac_explicit[:, idx] = torch.cross(joint_rotation[idx] @ joint.axis.data[:,0], pos_diff)\n",
    "                    # print('here')\n",
    "                elif isinstance(joint, Pjoint):\n",
    "                    pos_diff = EE_pos - joint_position[idx]\n",
    "                    jac_explicit[:,idx] = joint_rotation[idx] @ joint.axis.data[:,0]\n",
    "        \n",
    "        return jac_explicit\n",
    "\n",
    "    jac_explicit = kinematic_grad(model,q_values)\n",
    "    dp_dm = (jac_explicit @ dq_dm).numpy()\n",
    "    # print(dp_dm)\n",
    "\n",
    "end_time = time.time()\n",
    "explicit_time = end_time-start_time\n",
    "print(\"[Explicit]:{:2f}\".format(explicit_time))\n",
    "\n",
    "print(\"Result: [Explicit] is faster than [AutoGrad] by {:2f} times\".format(autograd_time/explicit_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 8.5829e-06, -5.5411e-06, -7.6270e-06,  1.1019e-05],\n",
       "        [ 3.4874e-05, -2.2515e-05, -3.0990e-05,  4.4772e-05],\n",
       "        [-1.7198e-05,  1.1103e-05,  1.5283e-05, -2.2079e-05],\n",
       "        [ 4.8425e-07, -3.1263e-07, -4.3031e-07,  6.2168e-07],\n",
       "        [ 1.2597e-05, -8.1328e-06, -1.1194e-05,  1.6172e-05],\n",
       "        [ 7.5818e-06, -4.8948e-06, -6.7374e-06,  9.7336e-06]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq_dm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
